#!/usr/bin/env python
# -*-coding:utf-8 -*-
import json
import logging
import re

import scrapy
from scrapy import Request, Selector

from ..items import VideoItem
from ..utils.logger_config import SpiderLoggerMixin


class ChiguaSpider(SpiderLoggerMixin, scrapy.Spider):
    name = "chigua"
    allowed_domains = []

    def __init__(self, *args, **kwargs):
        super(ChiguaSpider, self).__init__(*args, **kwargs)
        # ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—é…ç½®
        self.setup_spider_logger()

    def start_requests(self):
        for page in range(5, 15):
            for tag in ["èè‰", 'ä¸€çº¿å¤©', 'é¦’å¤´é€¼', 'ç¦åˆ©å§¬', 'è‡ªæ…°',
                        'ç¾ä¹³', 'ç²‰å«©', 'å­¦ç”Ÿå¦¹', 'é²é±¼', 'å«©ç©´', 'å°ç©´',
                        'å¤§å­¦', 'é«˜ä¸­', 'æµ·è§’']:
                yield Request(url=f'https://cabinet.byoeyvro.club/search/{tag}/{page}/')

    def parse(self, response):
        sel = Selector(response)
        chigua_titles = sel.xpath("//h2[@class='post-card-title' and @itemprop='headline']/text()").extract()
        chigua_hrefs = sel.xpath("//div[@id='archive']//a/@href").extract()

        for title, href in zip(chigua_titles, chigua_hrefs):
            item = VideoItem()
            item['title'] = title.strip()
            item['site'] = 'chigua'

            if "search" in href:
                continue

            complete_url = response.urljoin(href)  # Combining base url with href

            self.log(f"Requesting URL: {complete_url}", logging.INFO)
            item['href'] = complete_url
            yield Request(url=complete_url, callback=self.parse_details, meta={'item': item})

    def parse_details(self, response):
        """
        è§£æè¯¦æƒ…é¡µé¢ï¼Œæå–m3u8é“¾æ¥å¹¶è¿›è¡Œé¢„å»é‡å¤„ç†
        """
        item = response.meta['item']

        # å­˜å‚¨æ‰€æœ‰æ‰¾åˆ°çš„è§†é¢‘é“¾æ¥
        all_video_urls = []
        m3u8_urls = []

        # æå–è§†é¢‘é…ç½®æ•°æ®
        src_links = response.xpath("//div[@class='dplayer']/@data-config").extract()

        if src_links:
            for src in src_links:
                try:
                    config_data = json.loads(src)
                    video_url = config_data.get('video', {}).get('url', '')

                    if video_url:
                        all_video_urls.append(video_url)
                        # æ£€æŸ¥æ˜¯å¦ä¸ºm3u8æ ¼å¼çš„è§†é¢‘
                        if video_url.endswith('.m3u8') or 'm3u8' in video_url:
                            m3u8_urls.append(video_url)
                            self.log(f"âœ… æ‰¾åˆ°M3U8è§†é¢‘é“¾æ¥: {video_url}", logging.INFO)
                        else:
                            self.log(f"æ‰¾åˆ°å…¶ä»–æ ¼å¼è§†é¢‘é“¾æ¥: {video_url}", logging.INFO)

                except json.JSONDecodeError as e:
                    self.log(f"è§£æè§†é¢‘é…ç½®JSONå¤±è´¥: {e}", logging.ERROR)
                    continue

        # å¦‚æœæ²¡æ‰¾åˆ°é…ç½®æ•°æ®ï¼Œå°è¯•å…¶ä»–æ–¹å¼
        if not all_video_urls:
            self.log("é¡µé¢ä¸Šæœªæ‰¾åˆ°è§†é¢‘é…ç½®æ•°æ®ï¼Œå°è¯•å…¶ä»–æ–¹å¼...", logging.INFO)

            # å°è¯•å…¶ä»–å¯èƒ½çš„è§†é¢‘é“¾æ¥æå–æ–¹å¼
            # æŸ¥æ‰¾å¯èƒ½çš„m3u8é“¾æ¥
            m3u8_links = response.xpath("//source[@src[contains(., '.m3u8')]]/@src").extract()
            if not m3u8_links:
                # åœ¨è„šæœ¬æˆ–å…¶ä»–åœ°æ–¹æŸ¥æ‰¾m3u8é“¾æ¥
                m3u8_links = re.findall(r'["\']([^"\']*\.m3u8[^"\']*)["\']', response.text)

            for m3u8_link in m3u8_links:
                # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                if m3u8_link.startswith('http'):
                    video_url = m3u8_link
                else:
                    video_url = response.urljoin(m3u8_link)

                m3u8_urls.append(video_url)
                self.log(f"âœ… é€šè¿‡å…¶ä»–æ–¹å¼æ‰¾åˆ°M3U8é“¾æ¥: {video_url}", logging.INFO)

        # å¯¹m3u8 URLåˆ—è¡¨è¿›è¡Œé¢„å»é‡
        if m3u8_urls:
            unique_m3u8_urls = self._deduplicate_m3u8_urls(m3u8_urls)

            if unique_m3u8_urls:
                # æ ¹æ®å»é‡åçš„URLæ•°é‡å†³å®šå¦‚ä½•è®¾ç½®item
                if len(unique_m3u8_urls) == 1:
                    item['m3u8_url'] = unique_m3u8_urls[0]
                else:
                    item['m3u8_url'] = unique_m3u8_urls
                    self.log(f"ğŸ¯ å‡†å¤‡å‘é€itemåˆ°pipeline: {item['title']}")
                self.log(f"ğŸ“º å»é‡åM3U8é“¾æ¥æ•°é‡: {len(unique_m3u8_urls)}")
                yield item
            else:
                self.log(f"âŒ æ‰€æœ‰M3U8é“¾æ¥éƒ½æ˜¯é‡å¤çš„: {item['title']}", logging.WARNING)
        else:
            self.log(f"âŒ æœªæ‰¾åˆ°ä»»ä½•M3U8è§†é¢‘é“¾æ¥: {item['title']}", logging.WARNING)

    def _deduplicate_m3u8_urls(self, m3u8_urls):
        """
        å¯¹m3u8 URLåˆ—è¡¨è¿›è¡Œé¢„å»é‡
        """
        unique_urls = []
        seen_urls = set()
        seen_url_keys = set()

        for url in m3u8_urls:
            if not url or not isinstance(url, str):
                continue

            # URLæ ‡å‡†åŒ–
            normalized_url = url.strip()

            # å®Œå…¨ç›¸åŒçš„URLå»é‡
            if normalized_url in seen_urls:
                self.log(f"ğŸ”„ å‘ç°é‡å¤URLï¼Œè·³è¿‡: {normalized_url[:100]}...")
                continue

            # æå–URLå…³é”®ç‰¹å¾è¿›è¡Œç›¸ä¼¼æ€§å»é‡
            url_key = self._extract_url_key(normalized_url)
            if url_key in seen_url_keys:
                self.log(f"ğŸ”„ å‘ç°ç›¸ä¼¼URLï¼Œè·³è¿‡: {normalized_url[:100]}...")
                continue

            # éªŒè¯URLæ ¼å¼
            if not self._is_valid_m3u8_url(normalized_url):
                self.log(f"âŒ æ— æ•ˆçš„M3U8 URLï¼Œè·³è¿‡: {normalized_url[:100]}...")
                continue

            # é€šè¿‡æ‰€æœ‰æ£€æŸ¥ï¼Œæ·»åŠ åˆ°å”¯ä¸€åˆ—è¡¨
            unique_urls.append(normalized_url)
            seen_urls.add(normalized_url)
            seen_url_keys.add(url_key)

        self.log(f"ğŸ“Š URLå»é‡ç»Ÿè®¡: åŸå§‹æ•°é‡={len(m3u8_urls)}, å»é‡åæ•°é‡={len(unique_urls)}")
        return unique_urls

    def _extract_url_key(self, url):
        """
        ä»URLä¸­æå–å…³é”®æ ‡è¯†ç¬¦ç”¨äºé‡å¤æ£€æµ‹
        """
        import re

        # æ–¹æ³•1ï¼šæå–é•¿çš„å­—æ¯æ•°å­—ç»„åˆ
        matches = re.findall(r'[a-zA-Z0-9]{8,}', url)
        if matches:
            longest_match = max(matches, key=len)
            if len(longest_match) >= 12:
                return longest_match

        # æ–¹æ³•2ï¼šæå–è·¯å¾„ä¸­çš„å…³é”®éƒ¨åˆ†
        path_matches = re.findall(r'/([a-zA-Z0-9]{6,})/', url)
        if path_matches:
            return path_matches[-1]

        # æ–¹æ³•3ï¼šæå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        filename_match = re.search(r'/([^/]+)\.m3u8', url)
        if filename_match:
            filename = filename_match.group(1)
            if len(filename) >= 6:
                return filename

        # æ–¹æ³•4ï¼šè¿”å›URLçš„å“ˆå¸Œå€¼
        import hashlib
        return hashlib.md5(url.encode()).hexdigest()[:16]

    def _is_valid_m3u8_url(self, url):
        """
        éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„m3u8 URL
        """
        if not url:
            return False

        # åŸºæœ¬æ ¼å¼æ£€æŸ¥
        if not (url.startswith('http://') or url.startswith('https://')):
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«m3u8
        if not ('.m3u8' in url.lower() or 'm3u8' in url.lower()):
            return False

        # æ£€æŸ¥URLé•¿åº¦ï¼ˆè¿‡çŸ­çš„URLå¯èƒ½æ— æ•ˆï¼‰
        if len(url) < 20:
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„æ— æ•ˆå­—ç¬¦
        invalid_chars = ['<', '>', '"', "'", '\\']
        if any(char in url for char in invalid_chars):
            return False

        return True

    def closed(self, reason):
        self.log(f"çˆ¬è™«å…³é—­ï¼ŒåŸå› : {reason}", logging.INFO)
