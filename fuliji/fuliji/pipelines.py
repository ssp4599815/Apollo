# !/usr/bin/env python
# -*-coding:utf-8 -*-
import logging
import os
import subprocess
import concurrent.futures
import threading
import time

import requests
import scrapy
from scrapy.exceptions import DropItem
from scrapy.pipelines.images import ImagesPipeline
from scrapy.utils.project import get_project_settings


class ImgPipeline(ImagesPipeline):

    def get_media_requests(self, item, info):
        # è·å–å°†ç”¨äºå­˜å‚¨å›¾ç‰‡çš„ç›®å½•è·¯å¾„
        dir_path = self.get_directory_path(item)
        logging.info("Checking if directory exists at: %s" % dir_path)
        if not os.path.exists(dir_path):
            logging.info("Directory does not exist, downloading images")
            for index, image_url in enumerate(item['image_urls']):
                yield scrapy.Request(image_url, meta={'item': item, 'index': index})
        else:
            logging.info("Directory already exists, skipping download for images in: %s" % dir_path)

    def item_completed(self, results, item, info):
        image_paths = [x['path'] for ok, x in results if ok]
        # logging.info("image_paths: %s" % image_paths)
        if not image_paths:
            raise DropItem("No images downloaded %s" % image_paths)
        item['image_paths'] = image_paths
        # æ‰“å°å®Œç»“çš„æ—¥å¿—
        logging.info("Download images completed: %s" % item['title'])

        return item

    def file_path(self, request, response=None, info=None):
        item = request.meta['item']
        index = request.meta['index']
        url = request.url
        # è·å–å›¾ç‰‡æ ¼å¼
        image_format = url.split('.')[-1]
        file_name = f"{item['title']}-{index + 1}"  # ä»¥å›¾ç‰‡URLçš„é¡ºåºå‘½åæ–‡ä»¶
        # logging.info("file_name: %s" % file_name)
        return f"{item['title']}/{file_name}.{image_format}"

    def get_directory_path(self, item):
        """
        Returns the directory path for the given item.
        """
        settings = get_project_settings()
        images_store = settings.get('IMAGES_STORE', '')
        dir_path = os.path.join(images_store, item['site'], item['title'])
        return dir_path


class M3U8Pipeline:
    """
    ç”¨äºä¸‹è½½å’Œå¤„ç†M3U8è§†é¢‘æ–‡ä»¶çš„ç®¡é“
    æ”¯æŒå¹¶è¡Œä¸‹è½½å¤šä¸ªä¸åŒçš„m3u8è§†é¢‘æ–‡ä»¶ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰ä¸‹è½½å®Œæˆåæ‰å…³é—­çˆ¬è™«
    """

    def __init__(self):
        self.settings = get_project_settings()
        self.videos_store = self.settings.get('VIDEOS_STORE', 'videos')
        # FFmpegå¤šçº¿ç¨‹ä¸‹è½½é…ç½®
        self.max_threads = self.settings.get('FFMPEG_MAX_THREADS', 20)  # å•ä¸ªæ–‡ä»¶çš„çº¿ç¨‹æ•°
        # å¹¶è¡Œä¸‹è½½é…ç½®
        self.max_concurrent_downloads = self.settings.get('MAX_CONCURRENT_DOWNLOADS', 5)  # åŒæ—¶ä¸‹è½½çš„è§†é¢‘æ•°é‡
        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        if not os.path.exists(self.videos_store):
            os.makedirs(self.videos_store)
        
        # åˆ›å»ºçº¿ç¨‹æ± ç”¨äºå¹¶è¡Œä¸‹è½½
        self.download_executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=self.max_concurrent_downloads,
            thread_name_prefix="M3U8Download"
        )
        
        # æ­£åœ¨ä¸‹è½½çš„è§†é¢‘URLé›†åˆ
        self.downloading_urls = set()
        # æ´»åŠ¨ä¸‹è½½ä»»åŠ¡è®¡æ•°å™¨
        self.active_downloads = 0
        # æ´»åŠ¨ä¸‹è½½ä»»åŠ¡æ¡ä»¶å˜é‡ï¼ˆç”¨äºç­‰å¾…æ‰€æœ‰ä¸‹è½½å®Œæˆï¼‰
        self.downloads_done = threading.Condition()
        # çº¿ç¨‹é”
        self.lock = threading.RLock()
        # æ˜¯å¦å·²é€šçŸ¥çˆ¬è™«ç»“æŸ
        self.shutdown_notified = False
        
        logging.info(f"M3U8Pipelineåˆå§‹åŒ–å®Œæˆ - æœ€å¤§å¹¶è¡Œä¸‹è½½æ•°: {self.max_concurrent_downloads}, FFmpegçº¿ç¨‹æ•°: {self.max_threads}")
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self.monitor_thread = threading.Thread(target=self._monitor_downloads, daemon=True)
        self.monitor_thread.start()

    def process_item(self, item, spider):
        """
        å¤„ç†æ¯ä¸ªåŒ…å«m3u8_urlçš„item
        ç›´æ¥å¤„ç†ä¼ å…¥çš„itemï¼Œä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†å¤šä¸ªè§†é¢‘
        """
        # æ£€æŸ¥itemæ˜¯å¦åŒ…å«m3u8_urlå­—æ®µ
        if 'm3u8_url' not in item:
            logging.info(f"Item {item.get('title', 'Unknown')} æ²¡æœ‰m3u8_urlå­—æ®µï¼Œè·³è¿‡")
            return item

        m3u8_url = item['m3u8_url']
        title = item.get('title', 'Unknown')
        
        logging.info(f"M3U8Pipelineæ¥æ”¶åˆ°item: {title}")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ä¸‹è½½é˜Ÿåˆ—ä¸­
        with self.lock:
            if m3u8_url in self.downloading_urls:
                logging.info(f"è§†é¢‘ {title} å·²åœ¨ä¸‹è½½é˜Ÿåˆ—ä¸­ï¼Œè·³è¿‡")
                return item
            
            # æ ‡è®°ä¸ºæ­£åœ¨ä¸‹è½½
            self.downloading_urls.add(m3u8_url)
            # å¢åŠ æ´»åŠ¨ä¸‹è½½è®¡æ•°
            self.active_downloads += 1

        # è·å–å­˜å‚¨è·¯å¾„
        dir_path = self.get_directory_path(item)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
            logging.info(f"åˆ›å»ºç›®å½•: {dir_path}")

        # æäº¤åˆ°çº¿ç¨‹æ± å¼‚æ­¥ä¸‹è½½
        future = self.download_executor.submit(self._download_video_async, item, dir_path)
        
        # æ·»åŠ å®Œæˆå›è°ƒ
        future.add_done_callback(lambda f: self._download_completed(f, m3u8_url, title))
        
        logging.info(f"å·²æäº¤M3U8ä¸‹è½½ä»»åŠ¡åˆ°çº¿ç¨‹æ± : {title}")
        
        return item
    
    def _monitor_downloads(self):
        """
        ç›‘æ§çº¿ç¨‹ï¼Œæ£€æŸ¥å¹¶æ˜¾ç¤ºä¸‹è½½è¿›åº¦
        """
        last_count = -1
        while True:
            time.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
            with self.lock:
                if self.active_downloads != last_count:
                    last_count = self.active_downloads
                    if self.active_downloads > 0:
                        logging.info(f"å½“å‰æ­£åœ¨è¿›è¡Œçš„ä¸‹è½½ä»»åŠ¡: {self.active_downloads}")
                
                # å¦‚æœæ²¡æœ‰æ´»åŠ¨ä¸‹è½½ä¸”å·²ç»é€šçŸ¥è¿‡å…³é—­ï¼Œåˆ™é€€å‡ºç›‘æ§
                if self.active_downloads == 0 and self.shutdown_notified:
                    break

    def _download_video_async(self, item, dir_path):
        """
        å¼‚æ­¥ä¸‹è½½è§†é¢‘çš„å‡½æ•°
        """
        title = item.get('title', 'Unknown')
        try:
            logging.info(f"å¼€å§‹ä¸‹è½½è§†é¢‘: {title}")
            self.download_m3u8(item['m3u8_url'], dir_path, title)
            return {'success': True, 'title': title}
        except Exception as e:
            logging.error(f"å¼‚æ­¥ä¸‹è½½è§†é¢‘å¤±è´¥: {title}, é”™è¯¯: {e}")
            return {'success': False, 'title': title, 'error': str(e)}

    def _download_completed(self, future, m3u8_url, title):
        """
        ä¸‹è½½å®Œæˆçš„å›è°ƒå‡½æ•°
        """
        try:
            result = future.result()
            if result['success']:
                logging.info(f"âœ… M3U8è§†é¢‘ä¸‹è½½æˆåŠŸ: {title}")
            else:
                logging.error(f"âŒ M3U8è§†é¢‘ä¸‹è½½å¤±è´¥: {title}, é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        except Exception as e:
            logging.error(f"ä¸‹è½½å›è°ƒå¤„ç†å¤±è´¥: {title}, é”™è¯¯: {e}")
        finally:
            # ä»ä¸‹è½½é›†åˆä¸­ç§»é™¤å¹¶å‡å°‘æ´»åŠ¨ä¸‹è½½è®¡æ•°
            with self.lock:
                self.downloading_urls.discard(m3u8_url)
                self.active_downloads -= 1
                logging.info(f"è§†é¢‘ {title} å¤„ç†å®Œæˆï¼Œå‰©ä½™ä¸‹è½½ä»»åŠ¡: {self.active_downloads}")
                
                # å¦‚æœæ‰€æœ‰ä¸‹è½½éƒ½å®Œæˆäº†ï¼Œé€šçŸ¥æ¡ä»¶å˜é‡
                if self.active_downloads == 0:
                    with self.downloads_done:
                        self.downloads_done.notify_all()

    def download_m3u8(self, url, dir_path, title):
        """
        ä¸‹è½½m3u8æ–‡ä»¶åŠå…¶åˆ†ç‰‡
        å¦‚æœæ–‡ä»¶åå·²å­˜åœ¨ï¼Œä¼šè‡ªåŠ¨åœ¨æ–‡ä»¶ååæ·»åŠ é€’å¢æ•°å­—
        æ”¯æŒå¤šçº¿ç¨‹ä¸‹è½½ä»¥æé«˜ä¸‹è½½é€Ÿåº¦
        """
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
        output_filename = self.generate_unique_filename(dir_path, title, '.mp4')
        output_path = os.path.join(dir_path, output_filename)

        try:
            # æ–¹æ³•1: ä½¿ç”¨ffmpegå¤šçº¿ç¨‹ä¸‹è½½
            cmd = [
                'ffmpeg',
                '-allowed_extensions', 'ALL',  # å…è®¸æ‰€æœ‰æ‰©å±•
                '-threads', str(self.max_threads),  # è®¾ç½®çº¿ç¨‹æ•°
                '-http_seekable', '1',  # å¯ç”¨HTTPå¯å¯»å€
                '-user_agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',  # è®¾ç½®User-Agent
                '-i', url,  # è¾“å…¥URL
                '-c', 'copy',  # å¤åˆ¶æµï¼Œä¸é‡æ–°ç¼–ç 
                '-bsf:a', 'aac_adtstoasc',  # éŸ³é¢‘æµå¤„ç†
                '-threads', str(self.max_threads),  # è¾“å‡ºçº¿ç¨‹æ•°
                '-y',  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                output_path
            ]

            logging.info(f"ğŸš€ å¼€å§‹ä½¿ç”¨ffmpegå¤šçº¿ç¨‹ä¸‹è½½ (çº¿ç¨‹æ•°: {self.max_threads}): {title}")

            # æ‰§è¡Œå‘½ä»¤å¹¶æ•è·è¾“å‡º
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)  # 1å°æ—¶è¶…æ—¶

            if result.returncode == 0:
                logging.info(f"âœ… ä½¿ç”¨ffmpegå¤šçº¿ç¨‹ä¸‹è½½å®Œæˆ: {title}")
            else:
                logging.error(f"âŒ ffmpegä¸‹è½½å¤±è´¥ï¼Œè¿”å›ç : {result.returncode}, è§†é¢‘: {title}")
                logging.error(f"é”™è¯¯è¾“å‡º: {result.stderr}")
                raise subprocess.CalledProcessError(result.returncode, cmd)

        except subprocess.TimeoutExpired:
            logging.error(f"â° ffmpegä¸‹è½½è¶…æ—¶: {title}")
            raise Exception(f"ffmpegä¸‹è½½è¶…æ—¶: {title}")

        except Exception as e:
            logging.error(f"âŒ ffmpegä¸‹è½½å¤±è´¥: {title}, é”™è¯¯: {e}")

            try:
                # æ–¹æ³•2: å¦‚æœffmpegå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä¸‹è½½m3u8æ–‡ä»¶
                m3u8_filename = self.generate_unique_filename(dir_path, title, '.m3u8')
                m3u8_file_path = os.path.join(dir_path, m3u8_filename)

                logging.info(f"ğŸ”„ å°è¯•å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥ä¸‹è½½m3u8æ–‡ä»¶ - {title}")
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }

                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()

                with open(m3u8_file_path, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                logging.info(f"âœ… å·²ä¿å­˜m3u8æ–‡ä»¶: {title}")

                # æ³¨æ„: è¿™é‡Œåªä¿å­˜äº†m3u8æ–‡ä»¶æœ¬èº«ï¼Œæ²¡æœ‰ä¸‹è½½è§†é¢‘åˆ†ç‰‡
                # å®Œæ•´å®ç°åº”è¯¥è§£æm3u8æ–‡ä»¶ï¼Œä¸‹è½½æ‰€æœ‰åˆ†ç‰‡å¹¶åˆå¹¶

            except Exception as e2:
                logging.error(f"âŒ ä¿å­˜m3u8æ–‡ä»¶å¤±è´¥: {title}, é”™è¯¯: {e2}")
                raise Exception(f"æ— æ³•ä¸‹è½½m3u8æ–‡ä»¶: {title}")

    def generate_unique_filename(self, dir_path, title, file_ext):
        """
        ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œå¦‚æœå·²å­˜åœ¨åŒåæ–‡ä»¶ï¼Œåœ¨æ–‡ä»¶ååæ·»åŠ é€’å¢æ•°å­—
        """
        # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸åˆæ³•å­—ç¬¦
        cleaned_title = self.clean_filename(title)
        base_filename = f"{cleaned_title}{file_ext}"

        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å›åŸå§‹æ–‡ä»¶å
        if not os.path.exists(os.path.join(dir_path, base_filename)):
            return base_filename

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ é€’å¢æ•°å­—
        counter = 1
        while True:
            new_filename = f"{cleaned_title}_{counter}{file_ext}"
            if not os.path.exists(os.path.join(dir_path, new_filename)):
                logging.info(f"æ–‡ä»¶ {base_filename} å·²å­˜åœ¨ï¼Œä½¿ç”¨æ–°æ–‡ä»¶å: {new_filename}")
                return new_filename
            counter += 1

    def clean_filename(self, filename):
        """
        æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤æˆ–æ›¿æ¢ä¸åˆæ³•å­—ç¬¦
        """
        # å®šä¹‰ä¸åˆæ³•å­—ç¬¦
        illegal_chars = ['<', '>', ':', '"', '/', '\\', '|', '?', '*']

        cleaned = filename
        for char in illegal_chars:
            cleaned = cleaned.replace(char, '_')

        # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹
        cleaned = cleaned.strip(' .')

        return cleaned

    def get_directory_path(self, item):
        """
        è¿”å›è§†é¢‘æ–‡ä»¶çš„å­˜å‚¨ç›®å½•è·¯å¾„
        """
        dir_path = os.path.join(self.videos_store, item.get('site', 'default'))
        return dir_path

    def close_spider(self, spider):
        """
        çˆ¬è™«å…³é—­æ—¶ç­‰å¾…æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆå†å…³é—­
        """
        logging.info("ğŸ”„ çˆ¬è™«å³å°†å…³é—­ï¼Œç­‰å¾…æ‰€æœ‰M3U8ä¸‹è½½ä»»åŠ¡å®Œæˆ...")
        
        with self.lock:
            # æ ‡è®°ä¸ºå·²é€šçŸ¥å…³é—­
            self.shutdown_notified = True
            
            # å¦‚æœæ²¡æœ‰æ´»åŠ¨ä¸‹è½½ï¼Œç›´æ¥å…³é—­
            if self.active_downloads == 0:
                logging.info("âœ… æ²¡æœ‰æ´»åŠ¨çš„ä¸‹è½½ä»»åŠ¡ï¼Œç›´æ¥å…³é—­çˆ¬è™«")
                self._cleanup_resources()
                return
            
            logging.info(f"â³ ç­‰å¾… {self.active_downloads} ä¸ªä¸‹è½½ä»»åŠ¡å®Œæˆ...")
        
        # ç­‰å¾…æ‰€æœ‰ä¸‹è½½å®Œæˆ
        with self.downloads_done:
            # è®¾ç½®æœ€é•¿ç­‰å¾…æ—¶é—´ï¼Œé˜²æ­¢æ— é™ç­‰å¾…
            self.downloads_done.wait(timeout=7200)  # æœ€å¤šç­‰å¾…2å°æ—¶
        
        self._cleanup_resources()
        logging.info("ğŸ‰ æ‰€æœ‰M3U8ä¸‹è½½ä»»åŠ¡å·²å®Œæˆï¼Œçˆ¬è™«å¯ä»¥å®‰å…¨å…³é—­")

    def _cleanup_resources(self):
        """
        æ¸…ç†èµ„æº
        """
        try:
            # å…³é—­çº¿ç¨‹æ± 
            if hasattr(self, 'download_executor'):
                self.download_executor.shutdown(wait=False)
                logging.info("ğŸ§¹ ä¸‹è½½çº¿ç¨‹æ± å·²å…³é—­")
        except Exception as e:
            logging.error(f"âŒ å…³é—­èµ„æºæ—¶å‡ºé”™: {e}")
