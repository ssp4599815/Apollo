#!/usr/bin/env python
# -*-coding:utf-8 -*-
import concurrent.futures
import logging
import os
import shutil
import subprocess
import threading
import time

import scrapy
from scrapy.exceptions import DropItem
from scrapy.pipelines.images import ImagesPipeline
from scrapy.utils.project import get_project_settings

from .utils.database import DownloadDatabase
from .utils.logger_config import PipelineLoggerMixin


class ImgPipeline(PipelineLoggerMixin, ImagesPipeline):

    def __init__(self, store_uri, download_func=None, settings=None):
        super(ImgPipeline, self).__init__(store_uri, download_func, settings)
        # è®¾ç½®pipelineä¸“å±æ—¥å¿—
        self.setup_pipeline_logger('img')

    def get_media_requests(self, item, info):
        # è·å–å°†ç”¨äºå­˜å‚¨å›¾ç‰‡çš„ç›®å½•è·¯å¾„
        dir_path = self.get_directory_path(item)
        self.log(f"Checking if directory exists at: {dir_path}")
        if not os.path.exists(dir_path):
            self.log(f"Directory does not exist, downloading images")
            for index, image_url in enumerate(item['image_urls']):
                yield scrapy.Request(image_url, meta={'item': item, 'index': index})
        else:
            self.log(f"Directory already exists, skipping download for images in: {dir_path}")

    def item_completed(self, results, item, info):
        image_paths = [x['path'] for ok, x in results if ok]
        if not image_paths:
            self.log(f"No images downloaded {image_paths}", logging.WARNING)
            raise DropItem(f"No images downloaded {image_paths}")
        item['image_paths'] = image_paths
        # æ‰“å°å®Œç»“çš„æ—¥å¿—
        self.log(f"Download images completed: {item['title']}")

        return item

    def file_path(self, request, response=None, info=None):
        item = request.meta['item']
        index = request.meta['index']
        url = request.url
        # è·å–å›¾ç‰‡æ ¼å¼
        image_format = url.split('.')[-1]
        file_name = f"{item['title']}-{index + 1}"  # ä»¥å›¾ç‰‡URLçš„é¡ºåºå‘½å
        # ç¡®ä¿å­—ç¬¦åˆæ³•ç”¨äºæ–‡ä»¶å
        file_name = self.sanitize_filename(file_name)
        self.log(f"Storing image at: {file_name}.{image_format}")
        return f"{item['site']}/{file_name}.{image_format}"

    def get_directory_path(self, item):
        # åŸºäºitemçš„'title'æ„å»ºå›¾ç‰‡å­˜å‚¨ç›®å½•è·¯å¾„
        settings = get_project_settings()
        images_store = settings.get('IMAGES_STORE')
        sanitized_title = self.sanitize_filename(item['title'])
        return os.path.join(images_store, item['site'], sanitized_title)

    def sanitize_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤æˆ–æ›¿æ¢ä¸ç¬¦åˆè¦æ±‚çš„å­—ç¬¦."""
        import re
        # ç§»é™¤/æ›¿æ¢æ–‡ä»¶åä¸­ä¸åˆæ³•çš„å­—ç¬¦
        sanitized_name = re.sub(r'[\\/*?:"<>|\s]', '_', filename)
        # é™åˆ¶æ–‡ä»¶åé•¿åº¦
        if len(sanitized_name) > 100:
            sanitized_name = sanitized_name[:100]
        return sanitized_name


class M3U8Pipeline(PipelineLoggerMixin):
    """
    æ”¹è¿›çš„M3U8è§†é¢‘ä¸‹è½½ç®¡é“ï¼Œæ”¯æŒï¼š
    1. æ–­ç‚¹ç»­ä¼ 
    2. ä¸´æ—¶æ–‡ä»¶ç®¡ç†  
    3. SQLiteæ•°æ®åº“è®°å½•
    4. è‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    5. æ’é™¤æŒ‡å®šæ ‡é¢˜çš„è§†é¢‘ä¸‹è½½
    """

    def __init__(self):
        # è®¾ç½®pipelineä¸“å±æ—¥å¿—
        self.setup_pipeline_logger('m3u8')

        self.settings = get_project_settings()
        self.videos_store = self.settings.get('VIDEOS_STORE', 'videos')

        # ä¸´æ—¶æ–‡ä»¶ç›®å½•
        self.temp_store = os.path.join(os.path.dirname(self.videos_store), 'temp_downloads')

        # æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        self.db_path = self.settings.get('DATABASE_PATH', './data/downloads.db')

        # FFmpegå¤šçº¿ç¨‹ä¸‹è½½é…ç½®
        self.max_threads = self.settings.get('FFMPEG_MAX_THREADS', 20)
        # å¹¶è¡Œä¸‹è½½é…ç½®
        self.max_concurrent_downloads = self.settings.get('MAX_CONCURRENT_DOWNLOADS', 5)

        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        for directory in [self.videos_store, self.temp_store]:
            if not os.path.exists(directory):
                os.makedirs(directory)
                self.log(f"åˆ›å»ºç›®å½•: {directory}")

        # åˆå§‹åŒ–æ•°æ®åº“
        self.db = DownloadDatabase(self.db_path)

        # åˆ›å»ºçº¿ç¨‹æ± ç”¨äºå¹¶è¡Œä¸‹è½½
        self.download_executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=self.max_concurrent_downloads,
            thread_name_prefix="M3U8Download"
        )

        # æ­£åœ¨ä¸‹è½½çš„è§†é¢‘URLé›†åˆ
        self.downloading_urls = set()
        # å·²å¤„ç†çš„è§†é¢‘URLé›†åˆ
        self.processed_urls = set()
        # æ­£åœ¨ä¸‹è½½çš„è§†é¢‘æ ‡é¢˜é›†åˆ
        self.downloading_titles = set()
        # å·²å¤„ç†çš„è§†é¢‘æ ‡é¢˜é›†åˆ
        self.processed_titles = set()
        # æ´»åŠ¨ä¸‹è½½ä»»åŠ¡è®¡æ•°å™¨
        self.active_downloads = 0
        # æ´»åŠ¨ä¸‹è½½ä»»åŠ¡æ¡ä»¶å˜é‡ï¼ˆç”¨äºç­‰å¾…æ‰€æœ‰ä¸‹è½½å®Œæˆï¼‰
        self.downloads_done = threading.Condition()
        # çº¿ç¨‹é”
        self.lock = threading.RLock()
        # æ˜¯å¦å·²é€šçŸ¥çˆ¬è™«ç»“æŸ
        self.shutdown_notified = False

        # åˆå§‹åŒ–æ’é™¤æ ‡é¢˜åˆ—è¡¨
        self.excluded_titles = self._load_excluded_titles()

        self.log(
            f"M3U8Pipelineåˆå§‹åŒ–å®Œæˆ - æœ€å¤§å¹¶è¡Œä¸‹è½½æ•°: {self.max_concurrent_downloads}, FFmpegçº¿ç¨‹æ•°: {self.max_threads}")
        self.log(f"è§†é¢‘å­˜å‚¨ç›®å½•: {self.videos_store}")
        self.log(f"ä¸´æ—¶æ–‡ä»¶ç›®å½•: {self.temp_store}")
        self.log(f"æ•°æ®åº“æ–‡ä»¶: {self.db_path}")
        self.log(f"å·²åŠ è½½æ’é™¤æ ‡é¢˜æ•°é‡: {len(self.excluded_titles)}")

        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self.monitor_thread = threading.Thread(target=self._monitor_downloads, daemon=True)
        self.monitor_thread.start()

    def _load_excluded_titles(self):
        """
        åŠ è½½ä» utils/chigua æ–‡ä»¶ä¸­æ’é™¤çš„æ ‡é¢˜åˆ—è¡¨
        """
        excluded_titles = set()
        chigua_path = os.path.join(os.path.dirname(__file__), 'utils', '51chigua.txt')

        if os.path.exists(chigua_path):
            try:
                with open(chigua_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):  # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                            excluded_titles.add(line)
                self.log(f"æˆåŠŸä» {chigua_path} åŠ è½½ {len(excluded_titles)} ä¸ªæ’é™¤æ ‡é¢˜")
            except Exception as e:
                self.log(f"åŠ è½½æ’é™¤æ ‡é¢˜æ–‡ä»¶å¤±è´¥: {e}")
        else:
            self.log(f"æ’é™¤æ ‡é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {chigua_path}")

        return excluded_titles

    def _is_title_excluded(self, title):
        """
        æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
        """
        return title in self.excluded_titles

    def _get_url_hash(self, url):
        """ç”ŸæˆURLçš„å”¯ä¸€å“ˆå¸Œæ ‡è¯†"""
        return self.db.get_url_hash(url)

    def _is_download_completed(self, url, title):
        """æ£€æŸ¥è§†é¢‘æ˜¯å¦å·²ç»ä¸‹è½½å®Œæˆ"""
        completed, file_path = self.db.is_download_completed(url)

        if completed:
            self.log(f"è§†é¢‘ {title} å·²ä¸‹è½½å®Œæˆï¼Œæ–‡ä»¶è·¯å¾„: {file_path}")
            return True

        return False

    def _get_temp_file_path(self, url, title):
        """è·å–ä¸´æ—¶æ–‡ä»¶è·¯å¾„"""
        # å…ˆå°è¯•ä»æ•°æ®åº“è·å–ç°æœ‰çš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        existing_temp_path = self.db.get_temp_file_path(url)
        if existing_temp_path and os.path.exists(existing_temp_path):
            return existing_temp_path

        # ç”Ÿæˆæ–°çš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        url_hash = self._get_url_hash(url)
        cleaned_title = self.clean_filename(title)
        temp_filename = f"{cleaned_title}_{url_hash}.tmp.mp4"
        return os.path.join(self.temp_store, temp_filename)

    def _get_final_file_path(self, item, title):
        """è·å–æœ€ç»ˆæ–‡ä»¶è·¯å¾„"""
        dir_path = self.get_directory_path(item)
        final_filename = self.generate_unique_filename(dir_path, title, '.mp4')
        return os.path.join(dir_path, final_filename)

    def process_item(self, item, spider):
        """
        å¤„ç†æ¯ä¸ªåŒ…å«m3u8_urlçš„item
        æ”¯æŒå•ä¸ªURLå’ŒURLåˆ—è¡¨ï¼Œè¿›è¡Œå¤šå±‚å»é‡æ£€æŸ¥
        """
        # æ£€æŸ¥itemæ˜¯å¦åŒ…å«m3u8_urlå­—æ®µ
        if 'm3u8_url' not in item:
            self.log(f"Item {item.get('title', 'Unknown')} æ²¡æœ‰m3u8_urlå­—æ®µï¼Œè·³è¿‡")
            return item

        m3u8_url_data = item['m3u8_url']
        title = item.get('title', 'Unknown')
        site = item.get('site', 'unknown')

        # ç»Ÿä¸€å¤„ç†ï¼šå°†å•ä¸ªURLè½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(m3u8_url_data, str):
            m3u8_urls = [m3u8_url_data]
        elif isinstance(m3u8_url_data, list):
            m3u8_urls = m3u8_url_data
        else:
            self.log(f"âŒ m3u8_urlå­—æ®µç±»å‹ä¸æ”¯æŒ: {type(m3u8_url_data)}")
            return item

        self.log(f"ğŸ¯ M3U8Pipelineæ¥æ”¶åˆ°item: {title}, URLæ•°é‡: {len(m3u8_urls)}")

        # ç¬¬ä¸€å±‚ï¼šæ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
        if self._is_title_excluded(title):
            self.log(f"âŒ è§†é¢‘ '{title}' åœ¨æ’é™¤åˆ—è¡¨ä¸­ï¼Œè·³è¿‡ä¸‹è½½")
            return item

        # å¯¹URLåˆ—è¡¨è¿›è¡Œå»é‡å¤„ç†
        unique_urls = self._deduplicate_urls(m3u8_urls, title)

        if not unique_urls:
            self.log(f"âŒ æ‰€æœ‰URLéƒ½å·²å¤„ç†è¿‡æˆ–é‡å¤ï¼Œè·³è¿‡: {title}")
            return item

        self.log(f"âœ… å»é‡åçš„URLæ•°é‡: {len(unique_urls)}")

        # å¤„ç†æ¯ä¸ªå”¯ä¸€çš„URL
        processed_count = 0
        for m3u8_url in unique_urls:
            if self._process_single_url(m3u8_url, title, site, item):
                processed_count += 1

        if processed_count > 0:
            self.log(f"âœ… æˆåŠŸå¤„ç†äº† {processed_count} ä¸ªURLï¼Œæ ‡é¢˜: {title}")
        else:
            self.log(f"âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•URLï¼Œæ ‡é¢˜: {title}")

        return item

    def _deduplicate_urls(self, m3u8_urls, title):
        """
        å¯¹m3u8_urlåˆ—è¡¨è¿›è¡Œå»é‡
        """
        unique_urls = []
        seen_urls = set()
        seen_url_keys = set()

        for url in m3u8_urls:
            if not url or not isinstance(url, str):
                continue

            # URLæ ‡å‡†åŒ–
            normalized_url = url.strip()

            # ç¬¬ä¸€æ­¥ï¼šå®Œå…¨ç›¸åŒçš„URLå»é‡
            if normalized_url in seen_urls:
                self.log(f"ğŸ”„ å‘ç°å®Œå…¨é‡å¤çš„URLï¼Œè·³è¿‡: {normalized_url[:100]}...")
                continue

            # ç¬¬äºŒæ­¥ï¼šæå–URLå…³é”®ç‰¹å¾è¿›è¡Œå»é‡
            url_key = self._extract_url_key(normalized_url)
            if url_key in seen_url_keys:
                self.log(f"ğŸ”„ å‘ç°ç›¸ä¼¼URLï¼Œè·³è¿‡: {normalized_url[:100]}...")
                continue

            # ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥æ˜¯å¦å·²åœ¨ä¼šè¯ä¸­å¤„ç†è¿‡
            with self.lock:
                if normalized_url in self.processed_urls:
                    self.log(f"ğŸ”„ URLå·²åœ¨æœ¬æ¬¡ä¼šè¯ä¸­å¤„ç†è¿‡ï¼Œè·³è¿‡: {normalized_url[:100]}...")
                    continue

                # æ£€æŸ¥ç›¸ä¼¼URL
                is_similar, similar_url = self._is_similar_url(normalized_url)
                if is_similar:
                    self.log(f"ğŸ”„ å‘ç°ç›¸ä¼¼URLå·²å¤„ç†ï¼Œè·³è¿‡: {normalized_url[:100]}...")
                    continue

            # ç¬¬å››æ­¥ï¼šæ£€æŸ¥æ•°æ®åº“
            if self.db.is_downloaded(normalized_url):
                self.log(f"ğŸ”„ URLå·²åœ¨æ•°æ®åº“ä¸­å­˜åœ¨ï¼Œè·³è¿‡: {normalized_url[:100]}...")
                with self.lock:
                    self.processed_urls.add(normalized_url)
                continue

            # ç¬¬äº”æ­¥ï¼šæ£€æŸ¥æ˜¯å¦åœ¨å½“å‰ä¸‹è½½é˜Ÿåˆ—ä¸­
            with self.lock:
                if normalized_url in self.downloading_urls:
                    self.log(f"ğŸ”„ URLå·²åœ¨ä¸‹è½½é˜Ÿåˆ—ä¸­ï¼Œè·³è¿‡: {normalized_url[:100]}...")
                    continue

            # é€šè¿‡æ‰€æœ‰æ£€æŸ¥ï¼Œæ·»åŠ åˆ°å”¯ä¸€åˆ—è¡¨
            unique_urls.append(normalized_url)
            seen_urls.add(normalized_url)
            seen_url_keys.add(url_key)

        return unique_urls

    def _process_single_url(self, m3u8_url, title, site, item):
        """
        å¤„ç†å•ä¸ªm3u8 URL
        """
        try:
            # æœ€åä¸€æ¬¡æ£€æŸ¥ï¼šç¡®ä¿URLä»ç„¶æœ‰æ•ˆ
            with self.lock:
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦åœ¨ä¸‹è½½é˜Ÿåˆ—ä¸­ï¼ˆé¿å…å¹¶å‘é—®é¢˜ï¼‰
                if m3u8_url in self.downloading_urls:
                    self.log(f"âŒ URLå·²åœ¨ä¸‹è½½é˜Ÿåˆ—ä¸­ï¼Œè·³è¿‡: {m3u8_url[:100]}...")
                    return False

                # æ ‡è®°ä¸ºæ­£åœ¨ä¸‹è½½
                self.downloading_urls.add(m3u8_url)
                self.downloading_titles.add(title)
                self.processed_urls.add(m3u8_url)
                self.processed_titles.add(title)

            # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼ˆé˜²æ­¢å¤šä¸ªURLä¸‹è½½åˆ°åŒä¸€æ–‡ä»¶ï¼‰
            safe_filename = self.sanitize_filename(title)

            # å¦‚æœæœ‰å¤šä¸ªURLï¼Œéœ€è¦æ·»åŠ åºå·
            url_hash = self._get_url_hash(m3u8_url)
            final_filename = f"{safe_filename}_{url_hash}"

            output_file = os.path.join(self.download_path, f"{final_filename}.mp4")

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(output_file):
                with self.lock:
                    self.downloading_urls.discard(m3u8_url)
                    self.downloading_titles.discard(title)
                self.log(f"âŒ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {output_file}")
                self.db.mark_as_downloaded(m3u8_url, output_file, "already_exists")
                return False

            # æäº¤ä¸‹è½½ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
            future = self.download_executor.submit(self.download_m3u8,
                                                   {'m3u8_url': m3u8_url, 'title': title, 'site': site},
                                                   output_file)
            self.download_tasks.append(future)

            self.log(f"âœ… å·²æ·»åŠ åˆ°ä¸‹è½½é˜Ÿåˆ—: {title} -> {m3u8_url[:100]}...")
            return True

        except Exception as e:
            self.log(f"âŒ å¤„ç†URLå¤±è´¥: {e}")
            # æ¸…ç†æ ‡è®°
            with self.lock:
                self.downloading_urls.discard(m3u8_url)
                self.downloading_titles.discard(title)
            return False

    def _get_url_hash(self, url):
        """
        ç”ŸæˆURLçš„çŸ­å“ˆå¸Œå€¼ï¼Œç”¨äºæ–‡ä»¶å
        """
        import hashlib
        return hashlib.md5(url.encode()).hexdigest()[:8]

    def _extract_url_key(self, url):
        """
        ä»URLä¸­æå–å…³é”®æ ‡è¯†ç¬¦ç”¨äºé‡å¤æ£€æµ‹
        å¢å¼ºç‰ˆæœ¬ï¼Œæå–æ›´å¤šç‰¹å¾
        """
        import re

        # æ–¹æ³•1ï¼šæå–é•¿çš„å­—æ¯æ•°å­—ç»„åˆ
        matches = re.findall(r'[a-zA-Z0-9]{8,}', url)
        if matches:
            # è¿”å›æœ€é•¿çš„åŒ¹é…é¡¹ä½œä¸ºå…³é”®æ ‡è¯†
            longest_match = max(matches, key=len)
            if len(longest_match) >= 12:  # è¶³å¤Ÿé•¿çš„æ ‡è¯†ç¬¦
                return longest_match

        # æ–¹æ³•2ï¼šæå–è·¯å¾„ä¸­çš„å…³é”®éƒ¨åˆ†
        # ä¾‹å¦‚: /video/abc123/playlist.m3u8 -> abc123
        path_matches = re.findall(r'/([a-zA-Z0-9]{6,})/', url)
        if path_matches:
            return path_matches[-1]  # å–æœ€åä¸€ä¸ªåŒ¹é…

        # æ–¹æ³•3ï¼šæå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        filename_match = re.search(r'/([^/]+)\.m3u8', url)
        if filename_match:
            filename = filename_match.group(1)
            if len(filename) >= 6:
                return filename

        # æ–¹æ³•4ï¼šå¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›URLçš„å“ˆå¸Œå€¼
        import hashlib
        return hashlib.md5(url.encode()).hexdigest()[:16]

    def _monitor_downloads(self):
        """
        ç›‘æ§çº¿ç¨‹ï¼Œæ£€æŸ¥å¹¶æ˜¾ç¤ºä¸‹è½½è¿›åº¦
        """
        last_count = -1
        while True:
            time.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
            with self.lock:
                if self.active_downloads != last_count:
                    last_count = self.active_downloads
                    if self.active_downloads > 0:
                        self.log(f"å½“å‰æ­£åœ¨è¿›è¡Œçš„ä¸‹è½½ä»»åŠ¡: {self.active_downloads}")

                # å¦‚æœæ²¡æœ‰æ´»åŠ¨ä¸‹è½½ä¸”å·²ç»é€šçŸ¥è¿‡å…³é—­ï¼Œåˆ™é€€å‡ºç›‘æ§
                if self.active_downloads == 0 and self.shutdown_notified:
                    break

    def _download_video_async(self, item, dir_path):
        """
        å¼‚æ­¥ä¸‹è½½è§†é¢‘çš„å‡½æ•°ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
        """
        title = item.get('title', 'Unknown')
        m3u8_url = item['m3u8_url']

        try:
            self.log(f"å¼€å§‹ä¸‹è½½è§†é¢‘: {title}")

            # è·å–ä¸´æ—¶æ–‡ä»¶è·¯å¾„å’Œæœ€ç»ˆæ–‡ä»¶è·¯å¾„
            temp_file_path = self._get_temp_file_path(m3u8_url, title)
            final_file_path = self._get_final_file_path(item, title)

            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸´æ—¶æ–‡ä»¶ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
            resume_download = os.path.exists(temp_file_path) and os.path.getsize(temp_file_path) > 0

            if resume_download:
                self.log(f"å‘ç°ä¸´æ—¶æ–‡ä»¶ï¼Œå°è¯•æ–­ç‚¹ç»­ä¼ : {title}")

            # ä¸‹è½½è§†é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            success = self.download_m3u8_with_resume(m3u8_url, temp_file_path, title, resume_download)

            if success:
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(temp_file_path)

                # ä¸‹è½½æˆåŠŸï¼Œç§»åŠ¨åˆ°æ­£å¼ç›®å½•
                shutil.move(temp_file_path, final_file_path)
                self.log(f"âœ… è§†é¢‘ä¸‹è½½å®Œæˆï¼Œå·²ç§»åŠ¨åˆ°: {final_file_path}")

                # æ›´æ–°æ•°æ®åº“è®°å½•
                self.db.update_download_status(m3u8_url, 'completed', final_file_path, file_size)

                # å°†æˆåŠŸä¸‹è½½çš„è§†é¢‘æ ‡é¢˜è¿½åŠ åˆ°æ’é™¤åˆ—è¡¨æ–‡ä»¶ä¸­
                self._append_to_excluded_list(title)

                return {'success': True, 'title': title, 'file_path': final_file_path}
            else:
                # ä¸‹è½½å¤±è´¥ï¼Œæ›´æ–°çŠ¶æ€
                self.db.update_download_status(m3u8_url, 'failed')
                self.log(f"âŒ è§†é¢‘ä¸‹è½½å¤±è´¥ï¼Œä¸´æ—¶æ–‡ä»¶ä¿ç•™: {temp_file_path}")
                return {'success': False, 'title': title}

        except Exception as e:
            # ä¸‹è½½å¼‚å¸¸ï¼Œæ›´æ–°çŠ¶æ€
            self.db.update_download_status(m3u8_url, 'error')
            self.log(f"å¼‚æ­¥ä¸‹è½½è§†é¢‘å¤±è´¥: {title}, é”™è¯¯: {e}")
            return {'success': False, 'title': title, 'error': str(e)}

    def _download_completed(self, future, m3u8_url, title):
        """
        ä¸‹è½½å®Œæˆçš„å›è°ƒå‡½æ•°
        """
        try:
            result = future.result()
            if result['success']:
                self.log(f"âœ… M3U8è§†é¢‘ä¸‹è½½æˆåŠŸ: {title}")
            else:
                self.log(f"âŒ M3U8è§†é¢‘ä¸‹è½½å¤±è´¥: {title}, é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        except Exception as e:
            self.log(f"ä¸‹è½½å›è°ƒå¤„ç†å¤±è´¥: {title}, é”™è¯¯: {e}")
        finally:
            # ä»ä¸‹è½½é›†åˆä¸­ç§»é™¤å¹¶å‡å°‘æ´»åŠ¨ä¸‹è½½è®¡æ•°
            with self.lock:
                self.downloading_urls.discard(m3u8_url)
                self.active_downloads -= 1
                self.log(f"è§†é¢‘ {title} å¤„ç†å®Œæˆï¼Œå‰©ä½™ä¸‹è½½ä»»åŠ¡: {self.active_downloads}")

                # å¦‚æœæ‰€æœ‰ä¸‹è½½éƒ½å®Œæˆäº†ï¼Œé€šçŸ¥æ¡ä»¶å˜é‡
                if self.active_downloads == 0:
                    with self.downloads_done:
                        self.downloads_done.notify_all()

    def download_m3u8_with_resume(self, url, temp_file_path, title, resume=False):
        """
        ä¸‹è½½m3u8æ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
        """
        try:
            # æ„å»ºffmpegå‘½ä»¤
            cmd = [
                'ffmpeg',
                '-allowed_extensions', 'ALL',  # å…è®¸æ‰€æœ‰æ‰©å±•
                '-threads', str(self.max_threads),  # è®¾ç½®çº¿ç¨‹æ•°
                # '-http_seekable', '1',  # å¯ç”¨HTTPå¯å¯»å€ (æ—§ç‰ˆæœ¬ffmpegä¸æ”¯æŒæ­¤å‚æ•°)
                '-user_agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',  # è®¾ç½®User-Agent
                '-i', url,  # è¾“å…¥URL
                '-c', 'copy',  # å¤åˆ¶æµï¼Œä¸é‡æ–°ç¼–ç 
                '-bsf:a', 'aac_adtstoasc',  # éŸ³é¢‘æµå¤„ç†
                '-threads', str(self.max_threads),  # è¾“å‡ºçº¿ç¨‹æ•°
            ]

            # å¦‚æœæ˜¯æ–­ç‚¹ç»­ä¼ ä¸”æ–‡ä»¶å­˜åœ¨ï¼Œåˆ™ä¸è¦†ç›–
            if not resume:
                cmd.append('-y')  # è¦†ç›–è¾“å‡ºæ–‡ä»¶

            cmd.append(temp_file_path)

            if resume:
                self.log(f"ğŸ”„ æ–­ç‚¹ç»­ä¼ ä¸‹è½½: {title}")
            else:
                self.log(f"ğŸš€ å¼€å§‹æ–°ä¸‹è½½ (çº¿ç¨‹æ•°: {self.max_threads}): {title}")

            # æ‰§è¡Œå‘½ä»¤å¹¶æ•è·è¾“å‡ºï¼ˆå…¼å®¹Python 3.6åŠä»¥ä¸‹ç‰ˆæœ¬ï¼‰
            result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True,
                                    timeout=3600)  # 1å°æ—¶è¶…æ—¶

            if result.returncode == 0:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦çœŸçš„ä¸‹è½½å®Œæˆï¼ˆæ–‡ä»¶å¤§å°å¤§äº0ï¼‰
                if os.path.exists(temp_file_path) and os.path.getsize(temp_file_path) > 0:
                    self.log(f"âœ… è§†é¢‘ä¸‹è½½å®Œæˆ: {title}")
                    return True
                else:
                    self.log(f"âŒ ä¸‹è½½å®Œæˆä½†æ–‡ä»¶æ— æ•ˆ: {title}")
                    return False
            else:
                self.log(f"âŒ ffmpegä¸‹è½½å¤±è´¥ï¼Œè¿”å›ç : {result.returncode}, è§†é¢‘: {title}")
                self.log(f"é”™è¯¯è¾“å‡º: {result.stderr}")
                return False

        except subprocess.TimeoutExpired:
            self.log(f"â° ffmpegä¸‹è½½è¶…æ—¶: {title}")
            return False

        except Exception as e:
            self.log(f"âŒ ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {title}, é”™è¯¯: {e}")
            return False

    def generate_unique_filename(self, dir_path, title, file_ext):
        """
        ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œå¦‚æœå·²å­˜åœ¨åŒåæ–‡ä»¶ï¼Œåœ¨æ–‡ä»¶ååæ·»åŠ é€’å¢æ•°å­—
        """
        # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸åˆæ³•å­—ç¬¦
        cleaned_title = self.clean_filename(title)
        base_filename = f"{cleaned_title}{file_ext}"

        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å›åŸå§‹æ–‡ä»¶å
        if not os.path.exists(os.path.join(dir_path, base_filename)):
            return base_filename

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ é€’å¢æ•°å­—
        counter = 1
        while True:
            new_filename = f"{cleaned_title}_{counter}{file_ext}"
            if not os.path.exists(os.path.join(dir_path, new_filename)):
                self.log(f"æ–‡ä»¶ {base_filename} å·²å­˜åœ¨ï¼Œä½¿ç”¨æ–°æ–‡ä»¶å: {new_filename}")
                return new_filename
            counter += 1

    def clean_filename(self, filename):
        """
        æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤æˆ–æ›¿æ¢ä¸åˆæ³•å­—ç¬¦
        """
        # å®šä¹‰ä¸åˆæ³•å­—ç¬¦
        illegal_chars = ['<', '>', ':', '"', '/', '\\', '|', '?', '*']

        cleaned = filename
        for char in illegal_chars:
            cleaned = cleaned.replace(char, '_')

        # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹
        cleaned = cleaned.strip(' .')

        return cleaned

    def get_directory_path(self, item):
        """
        è¿”å›è§†é¢‘æ–‡ä»¶çš„å­˜å‚¨ç›®å½•è·¯å¾„
        """
        dir_path = os.path.join(self.videos_store, item.get('site', 'default'))
        return dir_path

    def cleanup_temp_files(self):
        """
        æ¸…ç†å­¤ç«‹çš„ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
        """
        try:
            if not os.path.exists(self.temp_store):
                return

            temp_files = [f for f in os.listdir(self.temp_store) if f.endswith('.tmp.mp4')]

            # æ£€æŸ¥ä¸´æ—¶æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´ï¼Œåˆ é™¤è¶…è¿‡7å¤©çš„æ–‡ä»¶
            current_time = time.time()
            week_ago = current_time - (7 * 24 * 60 * 60)  # 7å¤©å‰

            cleaned_count = 0
            for temp_file in temp_files:
                temp_file_path = os.path.join(self.temp_store, temp_file)
                try:
                    file_mtime = os.path.getmtime(temp_file_path)
                    if file_mtime < week_ago:
                        os.remove(temp_file_path)
                        cleaned_count += 1
                        self.log(f"åˆ é™¤è¿‡æœŸä¸´æ—¶æ–‡ä»¶: {temp_file}")
                except Exception as e:
                    self.log(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {temp_file}: {e}")

            if cleaned_count > 0:
                self.log(f"æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸä¸´æ—¶æ–‡ä»¶")

        except Exception as e:
            self.log(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    def get_download_status(self):
        """
        è·å–ä¸‹è½½çŠ¶æ€ç»Ÿè®¡
        """
        stats = self.db.get_download_statistics()
        temp_files_count = 0

        if os.path.exists(self.temp_store):
            temp_files_count = len([f for f in os.listdir(self.temp_store) if f.endswith('.tmp.mp4')])

        stats['temp_files'] = temp_files_count
        stats['active_downloads'] = self.active_downloads

        return stats

    def close_spider(self, spider):
        """
        çˆ¬è™«å…³é—­æ—¶ç­‰å¾…æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆå†å…³é—­
        """
        self.log("ğŸ”„ çˆ¬è™«å³å°†å…³é—­ï¼Œç­‰å¾…æ‰€æœ‰M3U8ä¸‹è½½ä»»åŠ¡å®Œæˆ...")

        # æ˜¾ç¤ºå½“å‰ä¸‹è½½çŠ¶æ€
        status = self.get_download_status()
        self.log(f"ğŸ“Š ä¸‹è½½çŠ¶æ€ç»Ÿè®¡: {status}")

        with self.lock:
            # æ ‡è®°ä¸ºå·²é€šçŸ¥å…³é—­
            self.shutdown_notified = True

            # å¦‚æœæ²¡æœ‰æ´»åŠ¨ä¸‹è½½ï¼Œç›´æ¥å…³é—­
            if self.active_downloads == 0:
                self.log("âœ… æ²¡æœ‰æ´»åŠ¨çš„ä¸‹è½½ä»»åŠ¡ï¼Œç›´æ¥å…³é—­çˆ¬è™«")
                self._cleanup_resources()
                return

            self.log(f"â³ ç­‰å¾… {self.active_downloads} ä¸ªä¸‹è½½ä»»åŠ¡å®Œæˆ...")

        # ç­‰å¾…æ‰€æœ‰ä¸‹è½½å®Œæˆ
        with self.downloads_done:
            # è®¾ç½®æœ€é•¿ç­‰å¾…æ—¶é—´ï¼Œé˜²æ­¢æ— é™ç­‰å¾…
            self.downloads_done.wait(timeout=7200)  # æœ€å¤šç­‰å¾…2å°æ—¶

        # æ¸…ç†å­¤ç«‹çš„ä¸´æ—¶æ–‡ä»¶
        self.cleanup_temp_files()

        self._cleanup_resources()

        # æœ€ç»ˆçŠ¶æ€ç»Ÿè®¡
        final_status = self.get_download_status()
        self.log(f"ğŸ“Š æœ€ç»ˆä¸‹è½½çŠ¶æ€: {final_status}")
        self.log("ğŸ‰ æ‰€æœ‰M3U8ä¸‹è½½ä»»åŠ¡å·²å®Œæˆï¼Œçˆ¬è™«å¯ä»¥å®‰å…¨å…³é—­")

    def _cleanup_resources(self):
        """
        æ¸…ç†èµ„æº
        """
        try:
            # å…³é—­æ•°æ®åº“è¿æ¥
            if hasattr(self, 'db'):
                self.db.close()
                self.log("ğŸ—„ï¸ æ•°æ®åº“è¿æ¥å·²å…³é—­")

            # å…³é—­çº¿ç¨‹æ± 
            if hasattr(self, 'download_executor'):
                self.download_executor.shutdown(wait=False)
                self.log("ğŸ§¹ ä¸‹è½½çº¿ç¨‹æ± å·²å…³é—­")

        except Exception as e:
            self.log(f"âŒ å…³é—­èµ„æºæ—¶å‡ºé”™: {e}")

    def _append_to_excluded_list(self, title):
        """
        å°†æˆåŠŸä¸‹è½½çš„è§†é¢‘æ ‡é¢˜è¿½åŠ åˆ°æ’é™¤åˆ—è¡¨æ–‡ä»¶ä¸­
        """
        try:
            chigua_path = os.path.join(os.path.dirname(__file__), 'utils', '51chigua.txt')

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(chigua_path), exist_ok=True)

            # è¿½åŠ æ ‡é¢˜åˆ°æ–‡ä»¶ï¼ˆä¸åŒ…å«.mp4åç¼€ï¼‰
            with open(chigua_path, 'a', encoding='utf-8') as f:
                f.write(f"\n{title}")

            # åŒæ—¶æ·»åŠ åˆ°å†…å­˜ä¸­çš„æ’é™¤é›†åˆï¼Œé¿å…é‡å¤ä¸‹è½½
            self.excluded_titles.add(title)

            self.log(f"âœ… å·²å°†æ ‡é¢˜ '{title}' è¿½åŠ åˆ°æ’é™¤åˆ—è¡¨æ–‡ä»¶: {chigua_path}")

        except Exception as e:
            self.log(f"âŒ è¿½åŠ æ ‡é¢˜åˆ°æ’é™¤åˆ—è¡¨å¤±è´¥: {title}, é”™è¯¯: {e}")

    def get_url_dedup_stats(self):
        """
        è·å–URLå»é‡ç»Ÿè®¡ä¿¡æ¯
        """
        with self.lock:
            return {
                'downloading_urls': len(self.downloading_urls),
                'processed_urls': len(self.processed_urls),
                'downloading_titles': len(self.downloading_titles),
                'processed_titles': len(self.processed_titles)
            }
